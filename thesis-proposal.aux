\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{icdarComp2015}
\citation{icdarComp2015}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces These are two words with a handful of letters left out. It is not hard to guess the correct words, especially if told these are names. A computer, with access to a lexicon, can be far more effective at this guessing game than a human.\relax }}{1}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:wheel_of_fortune_example}{{1}{1}{These are two words with a handful of letters left out. It is not hard to guess the correct words, especially if told these are names. A computer, with access to a lexicon, can be far more effective at this guessing game than a human.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{icdar_segmentation2013}
\citation{icdar_segmentation2013}
\citation{sayres}
\citation{sayres}
\citation{Marti2001}
\citation{Marti2001}
\citation{Graves2009hmm}
\citation{Graves2009hmm}
\citation{icdarComp2015}
\citation{icdarComp2015}
\citation{Serrano2010}
\citation{Serrano2010}
\citation{Toselli2007}
\citation{Toselli2007}
\citation{Toselli2008}
\citation{Toselli2008}
\citation{Toselli2009}
\citation{Toselli2009}
\citation{Toselli2010}
\citation{Toselli2010}
\citation{Serrano2014}
\citation{Serrano2014}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Computer Assisted Transcription}{2}{subsection.2.1}}
\citation{Clawson2014}
\citation{Clawson2014}
\citation{Zagoris2015}
\citation{Zagoris2015}
\citation{Neudecker2010}
\citation{Neudecker2010}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A screenshot of a demo of Toselli et al's multimodal CAT system. The red line is drawn by the user to indicate the need to insert a word into the automatically obtained transcription.\relax }}{3}{figure.caption.2}}
\newlabel{fig:Toselli_multimodalCAT}{{2}{3}{A screenshot of a demo of Toselli et al's multimodal CAT system. The red line is drawn by the user to indicate the need to insert a word into the automatically obtained transcription.\relax }{figure.caption.2}{}}
\citation{Neudecker2010}
\citation{Neudecker2010}
\citation{Neudecker2010}
\citation{Neudecker2010}
\citation{Retsinas2015}
\citation{Retsinas2015}
\citation{Neudecker2010}
\citation{Neudecker2010}
\citation{Clawson2014}
\citation{Clawson2014}
\citation{Neudecker2010}
\citation{Neudecker2010}
\citation{Retsinas2015}
\citation{Retsinas2015}
\citation{Rodrıguez2008}
\citation{Rodrıguez2008}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Clawson's CAT system for tabular documents.\relax }}{4}{figure.caption.3}}
\newlabel{fig:ii}{{3}{4}{Clawson's CAT system for tabular documents.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Word Spotting}{4}{subsection.2.2}}
\citation{Rothacker2013}
\citation{Rothacker2013}
\citation{Rodrıguez2008}
\citation{Rodrıguez2008}
\citation{Rath2003}
\citation{Rath2003}
\citation{Marti2001}
\citation{Marti2001}
\citation{Bunke2004}
\citation{Bunke2004}
\citation{Aldavert2015}
\citation{Aldavert2015}
\citation{Almazan2014}
\citation{Almazan2014}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A screen shot of a character session for ``?'' from Neudecker and Tzadok's CAT system, taken directly from their report \cite  {Neudecker2010}. Notice how easy it is for a user to simply click on the erroneous classifications.\relax }}{5}{figure.caption.4}}
\newlabel{fig:carpet}{{4}{5}{A screen shot of a character session for ``?'' from Neudecker and Tzadok's CAT system, taken directly from their report \cite {Neudecker2010}. Notice how easy it is for a user to simply click on the erroneous classifications.\relax }{figure.caption.4}{}}
\citation{Rothacker2013}
\citation{Rothacker2013}
\citation{Fischer2012}
\citation{Fischer2012}
\citation{Almazan2012}
\citation{Almazan2012}
\citation{Almazan2014}
\citation{Almazan2014}
\@writefile{toc}{\contentsline {section}{\numberline {3}Thesis Statement}{6}{section.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Project Description}{6}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Spotting}{6}{subsection.4.1}}
\citation{Zagoris2015}
\citation{Zagoris2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Preprocessing}{7}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Lexicon}{7}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Tool}{7}{subsection.4.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Computation Server Tasks}{7}{subsubsection.4.4.1}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.4.1.1}Character n-gram spotting}{7}{paragraph.4.4.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The work-flow of the proposed CAT system. (a) Server character n-gram spotting task. (b) Server n-gram verification distribution. Pools results from the previous task and distributes them to users as they become available. (c) User n-gram verification task. Passes it's results both forward (towards transcription) and backward (incremental learning and relevance feedback). (d) Server lexicon look-up and scoring task. A regular expression is generated from all currently spotted n-grams (``en'' and ``ed'' in this example). The regular expression generates a list of possible transcriptions and a word spotting/recognition algorithm scores each of these on the image. If the number of words above some threshold is small (black words), they are passed to a user. (e) User transcription selection task. (f) Extracting new n-gram exemplars which are used for incremental learning.\relax }}{8}{figure.caption.5}}
\newlabel{fig:system_diagram}{{5}{8}{The work-flow of the proposed CAT system. (a) Server character n-gram spotting task. (b) Server n-gram verification distribution. Pools results from the previous task and distributes them to users as they become available. (c) User n-gram verification task. Passes it's results both forward (towards transcription) and backward (incremental learning and relevance feedback). (d) Server lexicon look-up and scoring task. A regular expression is generated from all currently spotted n-grams (``en'' and ``ed'' in this example). The regular expression generates a list of possible transcriptions and a word spotting/recognition algorithm scores each of these on the image. If the number of words above some threshold is small (black words), they are passed to a user. (e) User transcription selection task. (f) Extracting new n-gram exemplars which are used for incremental learning.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.4.1.2}Character n-gram verification distribution}{9}{paragraph.4.4.1.2}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.4.1.3}Lexicon look-up and scoring}{9}{paragraph.4.4.1.3}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.4.1.4}Incremental learning}{9}{paragraph.4.4.1.4}}
\citation{GW}
\citation{GW}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces A mock-up of what the user might see when verifying character n-gram spotting. The highlighted images are from the server spotting a particular n-gram. The red-boxed image has been selected by the user as it is a false-positive.\relax }}{10}{figure.caption.6}}
\newlabel{fig:userTask_spot}{{6}{10}{A mock-up of what the user might see when verifying character n-gram spotting. The highlighted images are from the server spotting a particular n-gram. The red-boxed image has been selected by the user as it is a false-positive.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}User Tasks}{10}{subsubsection.4.4.2}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.4.2.1}Character n-gram verification}{10}{paragraph.4.4.2.1}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.4.2.2}Select correct transcription}{10}{paragraph.4.4.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}An Example}{10}{subsection.4.5}}
\citation{Almazan2014}
\citation{Almazan2014}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces A mock-up of what the user might see when selecting a correct transcription. The transcription possibilities are returned from the server's lexicon look-up.\relax }}{11}{figure.caption.7}}
\newlabel{fig:userTask_trans}{{7}{11}{A mock-up of what the user might see when selecting a correct transcription. The transcription possibilities are returned from the server's lexicon look-up.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Preliminary Work}{11}{subsection.4.6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.1}Subword Spotting}{11}{subsubsection.4.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces An example of what the process of transcribing the word ``there'' might look like. (a) set of all ``there''s in the corpus. (b) ``there''s in which ``th'' was successfully spotted after spotting it in the corpus. (c) lexicon lookups for these words is not constrained enough yet. (d) ``there''s in which ``er'' and ``th'' were successfully spotted after spotting ``\texttt  {er}'' in the corpus. (e) lexicon lookups now return a short list a user could select the correct transcription from.\relax }}{12}{figure.caption.8}}
\newlabel{fig:flow_ex}{{8}{12}{An example of what the process of transcribing the word ``there'' might look like. (a) set of all ``there''s in the corpus. (b) ``there''s in which ``th'' was successfully spotted after spotting it in the corpus. (c) lexicon lookups for these words is not constrained enough yet. (d) ``there''s in which ``er'' and ``th'' were successfully spotted after spotting ``\texttt {er}'' in the corpus. (e) lexicon lookups now return a short list a user could select the correct transcription from.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.2}System Simulation}{12}{subsubsection.4.6.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces A simulation run with medium confidence in predicting the number of unspotted characters in a word. The chart is drawn so one can observe the progress of spotting as well as transcription, each category indicating the portion of words which have the given percent of their characters recognized by spottings (or indicating the portion of words transcribed). One will note that a large number of words never have more than 20\% of their characters spotted. This is due to two reasons: we are only using the 100 most frequent bigrams, and many of words in the corpus are numbers, which we are not spotting or transcribing in this simulation. The group of words which are between 40\% and 60\% spotted which are never transcribed are either out-of-vocabulary as well or are simply diabolic cases which have too many possible transcriptions given the limited number of bigrams we can spot. An example of this diabolic case is the words ``suffice.'' The only bigrams we spot that are present in the word are ``ic'' and ``ce'' and the number of seven letter words ending in ``ice'' is 26 for our lexicon, thus this word would never be ``transcribed'' in the simulation.\relax }}{13}{figure.caption.9}}
\newlabel{fig:fullSim}{{9}{13}{A simulation run with medium confidence in predicting the number of unspotted characters in a word. The chart is drawn so one can observe the progress of spotting as well as transcription, each category indicating the portion of words which have the given percent of their characters recognized by spottings (or indicating the portion of words transcribed). One will note that a large number of words never have more than 20\% of their characters spotted. This is due to two reasons: we are only using the 100 most frequent bigrams, and many of words in the corpus are numbers, which we are not spotting or transcribing in this simulation. The group of words which are between 40\% and 60\% spotted which are never transcribed are either out-of-vocabulary as well or are simply diabolic cases which have too many possible transcriptions given the limited number of bigrams we can spot. An example of this diabolic case is the words ``suffice.'' The only bigrams we spot that are present in the word are ``ic'' and ``ce'' and the number of seven letter words ending in ``ice'' is 26 for our lexicon, thus this word would never be ``transcribed'' in the simulation.\relax }{figure.caption.9}{}}
\citation{GW}
\citation{GW}
\citation{IAM}
\citation{IAM}
\citation{SUS}
\citation{SUS}
\citation{GW}
\citation{GW}
\citation{Smith}
\citation{Smith}
\@writefile{toc}{\contentsline {section}{\numberline {5}Validation}{14}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}User Study}{14}{subsection.5.1}}
\bibstyle{plainnat}
\bibdata{bib}
\bibcite{Aldavert2015}{{1}{2015}{{Aldavert et~al.}}{{Aldavert, Rusi\={n}ol, Toledo, and Llad\'{o}s}}}
\bibcite{Almazan2012}{{2}{2012}{{Almaz\'{a}n et~al.}}{{Almaz\'{a}n, Fern\'{a}ndez, Forn\'{e}s, Llados, and Valveny}}}
\bibcite{Almazan2014}{{3}{2014}{{Almaz\'{a}n et~al.}}{{Almaz\'{a}n, Gordo, Forn\'{e}s, and Valveny}}}
\bibcite{SUS}{{4}{1996}{{Brooke}}{{}}}
\bibcite{Bunke2004}{{5}{2004}{{Bunke et~al.}}{{Bunke, Bengio, and Vinciarelli}}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Thesis Schedule}{15}{section.6}}
\bibcite{Clawson2014}{{6}{2014}{{Clawson}}{{}}}
\bibcite{Fischer2012}{{7}{2012}{{Fischer et~al.}}{{Fischer, Keller, Frinken, and Bunke}}}
\bibcite{Graves2009hmm}{{8}{2009}{{Graves et~al.}}{{Graves, Liwicki, Fernandez, Bertolami, Bunke, and Schmidhuber}}}
\bibcite{Smith}{{9}{2011}{{Kennard et~al.}}{{Kennard, Barrett, and Sederberg}}}
\bibcite{GW}{{10}{2004}{{Lavrenko et~al.}}{{Lavrenko, Rath, and Manmatha}}}
\bibcite{IAM}{{11}{2002}{{Marti and Bunke}}{{}}}
\bibcite{Marti2001}{{12}{2001}{{Marti and Bunke}}{{}}}
\bibcite{Neudecker2010}{{13}{2010}{{Neudecker and Tzadok}}{{}}}
\bibcite{Rath2003}{{14}{2003}{{Rath and Manmatha}}{{}}}
\bibcite{Retsinas2015}{{15}{2015}{{Retsinas et~al.}}{{Retsinas, Gatos, Antonacopoulos, Louloudis, and Stamatopoulos}}}
\bibcite{Rodrıguez2008}{{16}{2008}{{Rodr{\i }guez. and Perronnin}}{{}}}
\bibcite{Toselli2009}{{17}{2009}{{Romero et~al.}}{{Romero, Toselli, Rodriguez, and Vidal}}}
\bibcite{Rothacker2013}{{18}{2013}{{Rothacker et~al.}}{{Rothacker, Rusi{\~{n}ol}, and Fink}}}
\bibcite{icdarComp2015}{{19}{2015}{{S\'{a}nchez et~al.}}{{S\'{a}nchez, Toselli, Romero, and Vidal}}}
\bibcite{Serrano2010}{{20}{2010}{{Serrano et~al.}}{{Serrano, Gim{\'e}nez, Sanchis, and Juan}}}
\bibcite{Serrano2014}{{21}{2014}{{Serrano et~al.}}{{Serrano, Gim\'{e}nez, Civera, Sanchis, and Juan}}}
\bibcite{icdar_segmentation2013}{{22}{2013}{{Stamatopoulos et~al.}}{{Stamatopoulos, Gatos, Louloudis, Pal, and Alaei}}}
\bibcite{Toselli2007}{{23}{2007}{{Toselli et~al.}}{{Toselli, Romero, Rodriguez, and Vidal}}}
\bibcite{Toselli2010}{{24}{2010}{{Toselli et~al.}}{{Toselli, Romero, Pastor, , and Vidal}}}
\bibcite{Toselli2008}{{25}{2008}{{Toselli et~al.}}{{Toselli, Romero, and Vidal}}}
\bibcite{sayres}{{26}{2003}{{Vinciarelli}}{{}}}
\bibcite{Zagoris2015}{{27}{2015}{{Zagoris et~al.}}{{Zagoris, Pratikakis, and Gatos}}}
